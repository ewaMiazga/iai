{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zadanie 6\n",
    "\n",
    "Celem ćwiczenia jest implementacja algorytmu Q-learning.\n",
    "\n",
    "Następnie należy stworzyć agenta rozwiązującego problem [Taxi](https://gymnasium.farama.org/environments/toy_text/taxi/). Problem dostępny jest w pakiecie **gym**.\n",
    "\n",
    "Punktacja (max 8 pkt):\n",
    "- Implementacja algorytmu Q-learning. [3 pkt]\n",
    "- Eksperymenty dla różnych wartości hiperparametrów [2 pkt]\n",
    "- Jakość kodu [1.5 pkt]\n",
    "- Wnioski [1.5 pkt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import pygame\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interfejs\n",
    "\n",
    "class QLearningSolver:\n",
    "    \"\"\"Class containing the Q-learning algorithm that might be used for different discrete environments.\"\"\"\n",
    "    def __init__(self,\n",
    "                 streets:any,\n",
    "                 learning_rate:float=0.1,\n",
    "                 gamma:float=0.9, #discount rate\n",
    "                 epsilon:float=0.1, #exploration rate\n",
    "                 ):\n",
    "        self.streets = streets\n",
    "        self.observation_space = streets.observation_space.n\n",
    "        self.action_space = streets.action_space.n\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "    def __call__(self, state:np.ndarray, action:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return Q-value of given state and action.\"\"\"\n",
    "        return self.q_table[state, action]\n",
    "\n",
    "    def update(self, state:np.ndarray, action:np.ndarray, next_state:np.ndarray, reward:float) -> None: #reward <0 to kara tak jakby\n",
    "        \"\"\"Update Q-value of given state and action.\"\"\"\n",
    "\n",
    "        val = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "\n",
    "        prev_q = (1-self.learning_rate) * self(state, action) # multiplied by previous reward \n",
    "\n",
    "        new_val = prev_q + self.learning_rate * val #dla 0.1 alg. w stopniu nieznacznym uwzględnia kolejny ruch\n",
    "        self.q_table[state, action] =  new_val\n",
    "\n",
    "    def get_best_action(self, state:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return action that maximizes Q-value for a given state.\"\"\"\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Elegant representation of Q-learning solver.\"\"\"\n",
    "\n",
    "    def learn(self, episodes):\n",
    "        temp = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.streets.reset()\n",
    "            done = False\n",
    "            trip_length = 0\n",
    "    \n",
    "            while not done and trip_length < 25:\n",
    "                random_value = random.uniform(0, 1)\n",
    "                if (random_value < self.epsilon):\n",
    "                    action = self.streets.action_space.sample() # Explore a random action\n",
    "                else:\n",
    "                    action = self.get_best_action(state) # Use the action with the highest q-value\n",
    "            \n",
    "                next_state, reward, done, info = self.streets.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "            \n",
    "                trip_length += 1\n",
    "                state = next_state\n",
    "            temp.append(trip_length)\n",
    "        for i in range(10):\n",
    "            print(np.mean(temp[1000 * i:1000 * (i+1)]))\n",
    "\n",
    "    def solve(self, initial_state):\n",
    "        self.q_table[initial_state]\n",
    "\n",
    "\n",
    "        lengths=[]\n",
    "        for tripnum in range(1, 11):\n",
    "            state = self.streets.reset()\n",
    "            self.streets.render()\n",
    "\n",
    "            done = False\n",
    "            trip_length = 0\n",
    "        \n",
    "            while not done and trip_length < 25:\n",
    "                action = self.get_best_action(state)\n",
    "                next_state, reward, done, info = self.streets.step(action)\n",
    "                clear_output(wait=True)\n",
    "                print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "                print(self.streets.render(mode='ansi'))\n",
    "                sleep(.2)\n",
    "                state = next_state\n",
    "                trip_length += 1\n",
    "            lengths.append(trip_length)\n",
    "\n",
    "            sleep(.2)\n",
    "        avg_len=sum(lengths)/10\n",
    "        return avg_len\n",
    "        \n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie modelu na 10 000 iteracjach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.997\n",
      "24.926\n",
      "24.596\n",
      "23.443\n",
      "21.518\n",
      "18.584\n",
      "17.252\n",
      "16.31\n",
      "15.749\n",
      "15.375\n"
     ]
    }
   ],
   "source": [
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.1 #exploration rate\n",
    "\n",
    "\n",
    "#streets.render()\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10 Step 12\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "12.3\n"
     ]
    }
   ],
   "source": [
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.5\n",
    "gamma:float=0.9 #discount rate\n",
    "epsilon:float=0.1 #exploration rate\n",
    "\n",
    "\n",
    "initial_state = streets.encode(1, 0, 2, 0)\n",
    "streets.s = initial_state\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)\n",
    "\n",
    "print(solver.solve(initial_state))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie modelu na 30 000 iteracjach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10 Step 10\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "13.4\n"
     ]
    }
   ],
   "source": [
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.5\n",
    "gamma:float=0.9 #discount rate\n",
    "epsilon:float=0.1 #exploration rate\n",
    "\n",
    "\n",
    "initial_state = streets.encode(1, 0, 2, 0)\n",
    "streets.s = initial_state\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(30000)\n",
    "\n",
    "print(solver.solve(initial_state))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wnioski"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate\n",
    "Parametr jest wykorzystywany do wartościowania jak szybko chcemy żeby algorytm się uczył. Jak bardzo wartościujemy nową wartość reward w porównaniu do poprzedniej\n",
    "* zbyt duże zwiększenie lub zmieniejszenie tego parametru prowadzi do wstrzymania procesu uczenia i podejmowania decyzji które wahają się wokół optymalnych, ale nie ostatecznie nie dają najbardziej optymalnej drogi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning rate = 0.0\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.0\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.1\n",
      "24.995\n",
      "24.929\n",
      "24.476\n",
      "23.158\n",
      "20.792\n",
      "17.271\n",
      "15.752\n",
      "14.842\n",
      "14.047\n",
      "13.992\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning rate = 0.1\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.5\n",
      "24.814\n",
      "18.264\n",
      "13.548\n",
      "13.107\n",
      "13.045\n",
      "13.056\n",
      "13.033\n",
      "13.146\n",
      "13.154\n",
      "12.979\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning rate = 0.5\") #optymalna\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.4\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma\n",
    "Parametr opowiada za odpowiednie wartościowanie(uwzględniając gamma jako wagę) najkorzystniejszego następnego ruchu, który może teraz wykonać alogrytm. Parametr odwierciedla horyzont wyborów, które podejmuje algorytm.\n",
    "* parametr jeśli będzie zbyt mały nie będzie wogóle uwzględniał lub uwzględniał pomijalnie reward za kolejny ruch, przez co algorytm będzie skupiony jedynie na najbliższym ruchu, który w globalnym spojrzeniu nie zawsze jest najlepszym wyborem(jest tak jakby optimum lokalnym).\n",
    "* jeśli będzie za duży będzie zbyt zoreintowany na przyszłych działaniach, dlaczego nie jest to zbyt dobre, ponieważ obecne wybory nie zawsze są znaczące dla znalezienia optymalnego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.0\n",
      "24.999\n",
      "25.0\n",
      "24.974\n",
      "24.966\n",
      "24.973\n",
      "24.992\n",
      "24.972\n",
      "24.988\n",
      "24.961\n",
      "24.979\n"
     ]
    }
   ],
   "source": [
    "print(\"Gamma = 0.0\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.0 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.6\n",
      "25.0\n",
      "24.924\n",
      "24.609\n",
      "23.685\n",
      "20.969\n",
      "17.88\n",
      "15.577\n",
      "14.801\n",
      "14.062\n",
      "13.629\n"
     ]
    }
   ],
   "source": [
    "print(\"Gamma = 0.6\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.9\n",
      "25.0\n",
      "24.892\n",
      "23.507\n",
      "20.126\n",
      "14.689\n",
      "13.266\n",
      "13.042\n",
      "13.157\n",
      "13.039\n",
      "13.044\n"
     ]
    }
   ],
   "source": [
    "print(\"Gamma = 0.9\") #optymalna\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=1.0 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon\n",
    "Parametr opowiada za wybór wartości(akcji), która jest ruchem taksówki na planszy.\n",
    "* jeśli mamy mały parametr to, dużej większości powinien być wybierany ruch najkorzystniejszy dla garacza.\n",
    "* parametr jest wykorzystywany do zwiększenie szybkości poszukiwania najkorzystniejszej drogi, ponieważ wybierany tylko\n",
    "czasem parametr może zwiększyć możliwości eksploracyjne algorytmu, przez co przyspieszyć zanjdywanie najkorzystniejszej drogi.\n",
    "* jednakże zbyt duża wartość parametru może przyczynić się do zbyt dużej randomizacji wyboru kolejnego ruchu przez co droga, którą wybiera taksówka przestaje być optymalna i znacznie się wydłuża."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.0\n",
      "24.999\n",
      "24.942\n",
      "24.493\n",
      "23.034\n",
      "20.467\n",
      "17.887\n",
      "15.721\n",
      "15.047\n",
      "14.056\n",
      "13.806\n"
     ]
    }
   ],
   "source": [
    "print(\"epsilon = 0.0\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.0 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.1\n",
      "25.0\n",
      "24.93\n",
      "24.603\n",
      "23.626\n",
      "21.283\n",
      "18.837\n",
      "17.132\n",
      "16.188\n",
      "15.774\n",
      "15.504\n"
     ]
    }
   ],
   "source": [
    "print(\"epsilon = 0.1\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.1 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.5\n",
      "25.0\n",
      "24.975\n",
      "24.911\n",
      "24.708\n",
      "24.233\n",
      "23.666\n",
      "23.265\n",
      "22.855\n",
      "22.874\n",
      "22.88\n"
     ]
    }
   ],
   "source": [
    "print(\"epsilon = 0.5\")\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "# Hyperparameters\n",
    "learning_rate:float=0.1\n",
    "gamma:float=0.6 #discount rate\n",
    "epsilon:float=0.5 #exploration rate\n",
    "\n",
    "solver = QLearningSolver(streets, learning_rate, gamma, epsilon)\n",
    "\n",
    "solver.learn(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a552de0a42a98a523e8f67ee9117f7b13e645144e58150911097f2b178b1554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
