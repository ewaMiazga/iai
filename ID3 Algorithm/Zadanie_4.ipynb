{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import mode\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "import random\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples): #+\n",
        "    if num_samples == 0 or class_count == 0:\n",
        "      return 0\n",
        "    p = class_count/num_samples    \n",
        "    return - p * np.log2(p)\n",
        "\n",
        "class Group: #+\n",
        "    def __init__(self, group_classes): #target \n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.group_classes.size # [0, 0, 2, 2, 1, 2]\n",
        "\n",
        "    def count_classes(self):\n",
        "        class_count = [0, 0, 0] \n",
        "        for i in self.group_classes:\n",
        "          if(i == 0):\n",
        "            class_count[0] += 1\n",
        "          elif(i == 1):\n",
        "            class_count[1] += 1\n",
        "          elif(i == 2):\n",
        "            class_count[2] += 1\n",
        "        return class_count\n",
        "\n",
        "    def group_entropy(self):\n",
        "        class_count = self.count_classes()\n",
        "        k = 0\n",
        "        sum = 0\n",
        "        while (k < len(class_count)):\n",
        "          sum += entropy_func(class_count[k], len(self))\n",
        "          k += 1\n",
        "        return sum\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    def predict(self, data):\n",
        "        if self.val is None:\n",
        "            if data[self.split_feature] < self.split_val:\n",
        "                return self.child_node_a.predict(data)\n",
        "            else:\n",
        "                return self.child_node_b.predict(data)\n",
        "        else:\n",
        "            return self.val\n",
        "\n",
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        return (group_a.entropy * len(group_a) + group_b.entropy * len(group_b)) / (len(group_a) + len(group_b))\n",
        "\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        return parent_group.entropy - self.get_split_entropy(child_group_a, child_group_b)\n",
        "\n",
        "    def get_feature_values(self, data, which_feature):\n",
        "      feature_values = np.transpose(data)\n",
        "      return feature_values[which_feature]\n",
        "\n",
        "    def group_feature_values(self, feature_values, divider, classes): \n",
        "      group_child_a = []\n",
        "      group_child_b = []\n",
        "      for val, gr in zip(feature_values, classes):\n",
        "        if val < divider:\n",
        "            group_child_a.append(gr)\n",
        "        else:\n",
        "            group_child_b.append(gr)\n",
        "      return group_child_a, group_child_b\n",
        "\n",
        "    def get_best_feature_split(self, data, which_feature, classes): #find best divider value, attribute\n",
        "        feature_values = self.get_feature_values(data, which_feature)\n",
        "\n",
        "        max_inf_gain = 0\n",
        "        div = 0\n",
        "        for x in feature_values:\n",
        "            (group_child_a, group_child_b) = self.group_feature_values(feature_values, x, classes) #dostajemy pogrupowane wartosci klas ze wzgledu na divider dla probki \n",
        "            divider_inf_gain = self.get_information_gain(Group(np.array(classes)), Group(np.array(group_child_a)), Group(np.array(group_child_b)))\n",
        "            if divider_inf_gain > max_inf_gain:\n",
        "              max_inf_gain = divider_inf_gain\n",
        "              div = x\n",
        "        return div, max_inf_gain\n",
        "\n",
        "    def get_best_split(self, data, classes): #return index feature with max infromation gain\n",
        "        max_inf_gain = 0\n",
        "        best_divider = 0\n",
        "        best_feature = 0\n",
        "\n",
        "        for k in range(len(np.transpose(data))): # dla kazdego atrybuty sprawdz ->>\n",
        "          divider, inf_gain = self.get_best_feature_split(data, k, classes)\n",
        "          if inf_gain > max_inf_gain:\n",
        "            best_feature = k\n",
        "            max_inf_gain = inf_gain\n",
        "            best_divider = divider\n",
        "        return best_divider, best_feature\n",
        "\n",
        "    def get_data_grouped(self, data, classes, which_feature, divider):\n",
        "      smaller_data = np.empty((0, 4), float)\n",
        "      smaller_classes = np.empty((0, 0), int)\n",
        "      bigger_data = np.empty((0, 4), float)\n",
        "      bigger_classes = np.empty((0, 0), int)\n",
        "\n",
        "      feature_values = self.get_feature_values(data, which_feature)\n",
        "      for ind, x in enumerate(feature_values):\n",
        "          if x < divider:\n",
        "                  smaller_data = np.vstack([smaller_data, data[ind]])\n",
        "                  smaller_classes = np.append(smaller_classes, classes[ind])\n",
        "          else:\n",
        "                  bigger_data = np.vstack([bigger_data, data[ind]])\n",
        "                  bigger_classes = np.append(bigger_classes, classes[ind])\n",
        "      return (smaller_data, smaller_classes, bigger_data, bigger_classes)\n",
        "\n",
        "    def build_tree(self, data, classes, depth=0):  #data #target_vlues\n",
        "        data = np.array(data)\n",
        "        classes = np.array(classes)\n",
        "\n",
        "        if self.max_depth == depth:\n",
        "          classes_group = Group(classes)\n",
        "          max_howMany = 0\n",
        "          max_class = 0\n",
        "          for id, howMany in enumerate(classes_group.count_classes()):\n",
        "              if howMany > max_howMany:\n",
        "                max_class = id\n",
        "                max_howMany = howMany\n",
        "          return Node(None, None, depth, None, None, max_class)\n",
        "\n",
        "        if np.unique(classes).shape[0] == 1:\n",
        "          return Node(None, None, depth, None, None, classes[0])\n",
        "\n",
        "        divider, feature_id = self.get_best_split(data, classes)\n",
        "        child_a_data, child_a_classes, child_b_data, child_b_classes = self.get_data_grouped(data, classes, feature_id, divider)\n",
        "\n",
        "        child_node_a = self.build_tree(child_a_data, child_a_classes, depth + 1)\n",
        "        child_node_b = self.build_tree(child_b_data, child_b_classes, depth + 1)\n",
        "\n",
        "        if depth == 0:\n",
        "            self.tree = Node(feature_id, divider, depth, child_node_a, child_node_b)\n",
        "            \n",
        "        else:\n",
        "            return Node(feature_id, divider, depth, child_node_a, child_node_b)\n",
        "\n",
        "    def how_works(self, data, classes, depth=0):\n",
        "        print(depth)\n",
        "        data = np.array(data)\n",
        "        classes = np.array(classes)\n",
        "\n",
        "        if self.max_depth == depth: #when depth is max, wygrywa klasa wiekszosciowa\n",
        "          classes_group = Group(classes)\n",
        "          max_howMany = 0\n",
        "          max_class = 0\n",
        "          for id, howMany in enumerate(classes_group.count_classes()):\n",
        "              if howMany > max_howMany:\n",
        "                max_class = id\n",
        "                max_howMany = howMany\n",
        "          print(f\"max depth: {max_class}\")\n",
        "          return Node(None, None, depth, None, None, max_class)\n",
        "\n",
        "        if np.unique(classes).shape[0] == 1: # when node is lisciem\n",
        "          print(f\"lisc: {classes[0]}\")\n",
        "          return Node(None, None, depth, None, None, classes[0])\n",
        "\n",
        "        #to nie koniec, wiec szukamy dalej wchodzimy dalej w drzewo\n",
        "        divider, feature_id = self.get_best_split(data, classes)\n",
        "        child_a_data, child_a_classes, child_b_data, child_b_classes = self.get_data_grouped(data, classes, feature_id, divider)\n",
        "\n",
        "        print(\"child node a depth + 1\")\n",
        "        child_node_a = self.how_works(child_a_data, child_a_classes, depth + 1)\n",
        "        print(\"child node b depth + 1\")\n",
        "        child_node_b = self.how_works(child_b_data, child_b_classes, depth + 1)\n",
        "\n",
        "        if depth == 0: # when we start \n",
        "            print(\"depth == 0\")\n",
        "            self.tree = Node(feature_id, divider, depth, child_node_a, child_node_b)\n",
        "            \n",
        "        else: #zaden z tych przypadkow, czyli ani nie lisc, ani nie doszlismy do konca max_depth, ani nie start, czyli wychodzenie z zaglebienia np lewostronnego i dalsze poszukiwania aby wyliczcy wszystkie bloki \n",
        "            print(\"pozostaly przypadek\")\n",
        "            return Node(feature_id, divider, depth, child_node_a, child_node_b) \n",
        "            \n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wyniki obliczen na podanym zbiorze\n",
        "Dla depth == 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start building tree\n",
            "end building tree\n",
            "[6.3 2.5 4.9 1.5] 2 1\n",
            "[6.8 3.  5.5 2.1] 2 2\n",
            "[6.4 2.8 5.6 2.2] 2 2\n",
            "[5.6 3.  4.1 1.3] 1 1\n",
            "[4.9 3.6 1.4 0.1] 0 0\n",
            "[6.  3.  4.8 1.8] 2 2\n",
            "[6.3 2.3 4.4 1.3] 1 1\n",
            "[4.4 3.2 1.3 0.2] 0 0\n",
            "[4.4 2.9 1.4 0.2] 0 0\n",
            "[5.5 2.6 4.4 1.2] 1 1\n",
            "[6.9 3.1 5.1 2.3] 2 2\n",
            "[5.5 4.2 1.4 0.2] 0 0\n",
            "[5.2 2.7 3.9 1.4] 1 1\n",
            "[6.5 3.  5.5 1.8] 2 2\n",
            "[7.7 3.  6.1 2.3] 2 2\n",
            "[2 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n",
            "[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n",
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)\n",
        "dc = DecisionTreeClassifier(3)\n",
        "prediction_values = []\n",
        "print(\"start building tree\")\n",
        "dc.build_tree(x_train, y_train)\n",
        "#dc.how_works(x_train, y_train)\n",
        "print(\"end building tree\")\n",
        "\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    print(sample, prediction, gt)\n",
        "    prediction_values.append(prediction)\n",
        "prediction_values = np.array(prediction_values)\n",
        "\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(prediction_values)\n",
        "print(y_test)\n",
        "\n",
        "score_pred = sklearn.metrics.accuracy_score(y_test, prediction_values)\n",
        "print(f\"Accuracy: {score_pred}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wyniki obliczen dla depth == 2\n",
        "\n",
        "Obserwujemy tutaj juz spadek jakości drzewa decyzyjnego. Nie spada znacząco, ponieważ tak na prawdę zostaje wyliczona nieprawdiłowa wartość tylko dla jednego liścia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6.3 2.5 4.9 1.5] 2 1\n",
            "[6.8 3.  5.5 2.1] 2 2\n",
            "[6.4 2.8 5.6 2.2] 2 2\n",
            "[5.6 3.  4.1 1.3] 1 1\n",
            "[4.9 3.6 1.4 0.1] 0 0\n",
            "[6.  3.  4.8 1.8] 1 2\n",
            "[6.3 2.3 4.4 1.3] 1 1\n",
            "[4.4 3.2 1.3 0.2] 0 0\n",
            "[4.4 2.9 1.4 0.2] 0 0\n",
            "[5.5 2.6 4.4 1.2] 1 1\n",
            "[6.9 3.1 5.1 2.3] 2 2\n",
            "[5.5 4.2 1.4 0.2] 0 0\n",
            "[5.2 2.7 3.9 1.4] 1 1\n",
            "[6.5 3.  5.5 1.8] 2 2\n",
            "[7.7 3.  6.1 2.3] 2 2\n",
            "[2 2 2 1 0 1 1 0 0 1 2 0 1 2 2]\n",
            "[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n",
            "Accuracy: 0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)\n",
        "dc = DecisionTreeClassifier(2)\n",
        "prediction_values = []\n",
        "dc.build_tree(x_train, y_train)\n",
        "\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    print(sample, prediction, gt)\n",
        "    prediction_values.append(prediction)\n",
        "prediction_values = np.array(prediction_values)\n",
        "\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(prediction_values)\n",
        "print(y_test)\n",
        "\n",
        "score_pred = sklearn.metrics.accuracy_score(y_test, prediction_values)\n",
        "print(f\"Accuracy: {score_pred}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wyniki obliczen dla podwyzszonego random_state\n",
        "Inne dane, a raczej wybrane inaczej, poniważ właśnie to definiuje random_state. Algorytm dalej jest skuteczny, co pokazuje ponad 80% Accurency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6.7 3.1 4.4 1.4] 1 1\n",
            "[4.8 3.1 1.6 0.2] 0 0\n",
            "[6.1 2.8 4.7 1.2] 1 1\n",
            "[4.8 3.4 1.9 0.2] 0 0\n",
            "[5.9 3.2 4.8 1.8] 2 1\n",
            "[6.3 2.5 5.  1.9] 2 2\n",
            "[6.8 3.  5.5 2.1] 2 2\n",
            "[5.4 3.7 1.5 0.2] 0 0\n",
            "[6.4 2.8 5.6 2.1] 2 2\n",
            "[6.5 3.2 5.1 2. ] 2 2\n",
            "[6.4 2.9 4.3 1.3] 1 1\n",
            "[7.7 3.8 6.7 2.2] 2 2\n",
            "[6.3 2.7 4.9 1.8] 2 2\n",
            "[6.7 3.  5.  1.7] 2 1\n",
            "[6.3 2.3 4.4 1.3] 1 1\n",
            "[1 0 1 0 2 2 2 0 2 2 1 2 2 2 1]\n",
            "[1 0 1 0 1 2 2 0 2 2 1 2 2 1 1]\n",
            "Accuracy: 0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=574) \n",
        "dc = DecisionTreeClassifier(3)\n",
        "prediction_values = []\n",
        "dc.build_tree(x_train, y_train)\n",
        "\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    print(sample, prediction, gt)\n",
        "    prediction_values.append(prediction)\n",
        "prediction_values = np.array(prediction_values)\n",
        "\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(prediction_values)\n",
        "print(y_test)\n",
        "\n",
        "score_pred = sklearn.metrics.accuracy_score(y_test, prediction_values)\n",
        "print(f\"Accuracy: {score_pred}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wyniki obliczen dla podwyzszonego test_size = 0.25, a wiec mniejsza ilosc zbioru zostala przebadana\n",
        "Algorytm zmniejszył swoją skuteczność. Jest to spowodowane mniejszą ilością danych, które dostało drzewo decyzyjne, aby nauczyć się jak powinnien klasyfikować próbki. Random_state pozostawiłam na tym samym poziomie, aby móc porównać wydajność algorytmu. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6.3 2.5 4.9 1.5] 1 1\n",
            "[6.8 3.  5.5 2.1] 2 2\n",
            "[6.4 2.8 5.6 2.2] 2 2\n",
            "[5.6 3.  4.1 1.3] 1 1\n",
            "[4.9 3.6 1.4 0.1] 0 0\n",
            "[6.  3.  4.8 1.8] 1 2\n",
            "[6.3 2.3 4.4 1.3] 1 1\n",
            "[4.4 3.2 1.3 0.2] 0 0\n",
            "[4.4 2.9 1.4 0.2] 0 0\n",
            "[5.5 2.6 4.4 1.2] 1 1\n",
            "[6.9 3.1 5.1 2.3] 2 2\n",
            "[5.5 4.2 1.4 0.2] 0 0\n",
            "[5.2 2.7 3.9 1.4] 1 1\n",
            "[6.5 3.  5.5 1.8] 2 2\n",
            "[7.7 3.  6.1 2.3] 2 2\n",
            "[6.5 3.  5.8 2.2] 2 2\n",
            "[5.5 3.5 1.3 0.2] 0 0\n",
            "[4.3 3.  1.1 0.1] 0 0\n",
            "[6.1 2.9 4.7 1.4] 1 1\n",
            "[4.8 3.  1.4 0.3] 0 0\n",
            "[5.2 3.4 1.4 0.2] 0 0\n",
            "[6.3 2.8 5.1 1.5] 1 2\n",
            "[4.8 3.4 1.9 0.2] 0 0\n",
            "[6.1 3.  4.9 1.8] 2 2\n",
            "[5.1 3.8 1.6 0.2] 0 0\n",
            "[5.4 3.4 1.7 0.2] 0 0\n",
            "[5.4 3.4 1.5 0.4] 0 0\n",
            "[5.6 2.8 4.9 2. ] 2 2\n",
            "[7.7 3.8 6.7 2.2] 2 2\n",
            "[5.  3.6 1.4 0.2] 0 0\n",
            "[7.4 2.8 6.1 1.9] 2 2\n",
            "[6.  2.2 5.  1.5] 1 2\n",
            "[4.7 3.2 1.6 0.2] 0 0\n",
            "[5.1 3.5 1.4 0.2] 0 0\n",
            "[6.  2.2 4.  1. ] 1 1\n",
            "[5.  2.3 3.3 1. ] 1 1\n",
            "[7.9 3.8 6.4 2. ] 2 2\n",
            "[5.4 3.9 1.7 0.4] 0 0\n",
            "Errors count: 3\n",
            "[1 2 2 1 0 1 1 0 0 1 2 0 1 2 2 2 0 0 1 0 0 1 0 2 0 0 0 2 2 0 2 1 0 0 1 1 2\n",
            " 0]\n",
            "[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2 2 0 0 1 0 0 2 0 2 0 0 0 2 2 0 2 2 0 0 1 1 2\n",
            " 0]\n",
            "Accuracy: 0.9210526315789473\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=123)\n",
        "errors_count = 0\n",
        "dc = DecisionTreeClassifier(3)\n",
        "prediction_values = []\n",
        "dc.build_tree(x_train, y_train)\n",
        "\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    if prediction != gt:\n",
        "        errors_count += 1\n",
        "    print(sample, prediction, gt)\n",
        "    prediction_values.append(prediction)\n",
        "print(f\"Errors count: {errors_count}\")\n",
        "prediction_values = np.array(prediction_values)\n",
        "\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(prediction_values)\n",
        "print(y_test)\n",
        "\n",
        "score_pred = sklearn.metrics.accuracy_score(y_test, prediction_values)\n",
        "print(f\"Accuracy: {score_pred}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3f75e0feec92dee3b009653d57e81dbac4950bad359ee4977b1aa3016ac007f2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
