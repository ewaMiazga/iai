{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zadanie 5\n",
    "\n",
    "\n",
    "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
    "\n",
    "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
    "\n",
    "Punktacja:\n",
    "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
    "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
    "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
    "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
    "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
    "    3. Wnioski 1.[5 pkt]\n",
    "4. Jakość kodu 0.[5 pkt]\n",
    "\n",
    "Polecane źródła - teoria + intuicja:\n",
    "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
    "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
    "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z: np.ndarray)->np.ndarray:\n",
    "    A = np.maximum(0.0, Z)\n",
    "    return A, Z\n",
    "\n",
    "def relu_prime(Z: np.ndarray)->np.ndarray:\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "def tanh(Z:np.ndarray)->np.ndarray:\n",
    "    A = np.tanh(Z)\n",
    "    return A\n",
    "\n",
    "def tanh_prime(Z:np.ndarray)->np.ndarray:\n",
    "    P = 1 - np.tanh(Z) ** 2\n",
    "    return P\n",
    "\n",
    "\n",
    "#a = np.arange(5)\n",
    "#print(a)\n",
    "#print(reLU(a))\n",
    "#print(reLU_prime(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "###loss function\n",
    "\n",
    "def loss_function(x, y):\n",
    "    return np.mean(pow((x - y), 2))\n",
    "\n",
    "def loss_function_derivative(x, y):\n",
    "    return 2 * (x - y) / y.size\n",
    "\n",
    "#'''def loss_function(x, y):\n",
    "#    return - np.sum(y * np.log(x))\n",
    "\n",
    "#def loss_function_derivative(x, y):\n",
    "#    return '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._learning_rate = 0.01\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x:np.ndarray)->np.ndarray: #\n",
    "        \"\"\"Forward propagation of x through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error_derivative) ->np.ndarray:\n",
    "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "\n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self, learning_rate):\n",
    "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
    "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    def __init__(self, input_size:int, output_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size #size of previous array\n",
    "        self.output_size = output_size #size of current array\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.generate_biases()\n",
    "        self.generate_weigths()\n",
    "\n",
    "    def generate_weigths(self):\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size) / np.sqrt(self.input_size)\n",
    "    \n",
    "    def generate_biases(self):\n",
    "        self.biases = np.random.randn(1, self.output_size)\n",
    "\n",
    "    def forward(self, input:np.ndarray)->np.ndarray:\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.input, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_error_derivative)->np.ndarray:\n",
    "        input_error = np.dot(output_error_derivative, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error_derivative)\n",
    "\n",
    "        #gradient descent\n",
    "        self.weights -= self.learning_rate * weights_error\n",
    "        self.biases -= self.learning_rate * output_error_derivative\n",
    "\n",
    "        return input_error\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input:np.ndarray)->np.ndarray:\n",
    "        self.input = input\n",
    "        return tanh(self.input)\n",
    "\n",
    "    def backward(self, output_error_derivative)->np.ndarray:\n",
    "        return tanh_prime(self.input) * output_error_derivative\n",
    "\n",
    "class reLu(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input:np.ndarray)->np.ndarray:\n",
    "        self.input = input\n",
    "        return relu(self.input)\n",
    "\n",
    "    def backward(self, output_error_derivative)->np.ndarray:\n",
    "        return relu_prime(self.input) * output_error_derivative #lack of argument of tanh_prime\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "\n",
    "    def loss(self, input:np.ndarray, y:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Loss function for a particular input\"\"\"\n",
    "        return self.loss_function(input, y)\n",
    "        \"\"\"sum = 0\n",
    "        for x, dis in zip(input, y):\n",
    "            sum += self.loss_function(x, dis)\n",
    "        return sum\"\"\"\n",
    "\n",
    "\n",
    "    def loss_derivative(self, input:np.ndarray, y:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
    "        \"\"\"sum = 0\n",
    "        for x, dis in zip(input, y):\n",
    "            sum += self.loss_derivative(x, dis)\"\"\"\n",
    "        return self.loss_function_derivative(input, y)\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss:Loss, input:np.ndarray, y:np.ndarray)->None: #funkcja straty i jej pochodną\n",
    "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
    "        return loss.loss(input, y), loss.loss_derivative(input, y)\n",
    "\n",
    "    def __call__(self, input:np.ndarray) -> np.ndarray: #funkcje forward, zwraca funkcje wyjścia\n",
    "        \"\"\"Forward propagation of input through all layers\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def predict(self, input:np.ndarray):\n",
    "        samples = len(input)\n",
    "        result = []\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = self.__call__(input[i])\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    def fit(self,\n",
    "            x_train:np.ndarray,\n",
    "            y_train:np.ndarray,\n",
    "            epochs:int,\n",
    "            learning_rate:float,\n",
    "            loss_function:Loss,\n",
    "            verbose:int=0)->None:\n",
    "        \"\"\"Fit the network to the training data\"\"\"\n",
    "        samples = x_train.shape[0]\n",
    "\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "\n",
    "            for j in range(samples):\n",
    "                input = x_train[j]\n",
    "                output = self.__call__(input)\n",
    "\n",
    "                loss += self.compile(loss_function, output, y_train[j])[0]\n",
    "\n",
    "                #backward propagation\n",
    "                loss_d = self.compile(loss_function, output, y_train[j])[1]\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    loss_d = layer.backward(loss_d)\n",
    "\n",
    "            loss /= samples\n",
    "            if i == 0 or i == epochs - 1:\n",
    "                print('epoch %d/%d   error=%f' % (i+1, epochs, loss))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.428239\n",
      "epoch 1000/1000   error=0.011713\n",
      "[array([[0.03286256]]), array([[0.851166]]), array([[0.84956767]]), array([[0.02208933]])]\n",
      "[0, 1, 1, 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "y_get = [0, 1, 1, 0]\n",
    "\n",
    "# network\n",
    "loss = Loss(loss_function, loss_function_derivative)\n",
    "\n",
    "net = Network([], 0.01)\n",
    "net.add(FullyConnected(2, 3))\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(3, 1))\n",
    "net.add(Tanh())\n",
    "\n",
    "net.fit(x_train, y_train, 1000, 0.1, loss)\n",
    "\n",
    "\n",
    "out = net.predict(x_train)\n",
    "\n",
    "predicted_values = []\n",
    "for el in out:\n",
    "    if el >= 0.5:\n",
    "        predicted_values.append(1)\n",
    "    else:\n",
    "        predicted_values.append(0)\n",
    "print(out)\n",
    "print(predicted_values)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_get, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Eksperymenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73812\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "loss = Loss(loss_function, loss_function_derivative)\n",
    "\n",
    "net = Network([], 0.01)\n",
    "net.add(FullyConnected(28*28, 100))\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(100, 50))\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(50, 10))\n",
    "\n",
    "net.add(Tanh())\n",
    "\n",
    "#net.fit(x_train[0:1000], y_train[0:1000], 35, 0.01, loss)\n",
    "\n",
    "# test on samples\n",
    "out = net.predict(x_test)\n",
    "\n",
    "predicted_values = []\n",
    "for sample in out:\n",
    "    for num in sample:\n",
    "        for el in num:\n",
    "            if el >= 0.5:\n",
    "                predicted_values.append(1)\n",
    "            else:\n",
    "                predicted_values.append(0)\n",
    "\n",
    "y_values = []\n",
    "for sample in y_test:\n",
    "    for el in sample:\n",
    "        if el == 1:\n",
    "            y_values.append(1)\n",
    "        else:\n",
    "            y_values.append(0)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_values, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare reLU with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reLU: \n",
      "epoch 1/35   error=0.107785\n",
      "epoch 35/35   error=0.020956\n",
      "Accuracy: 0.73812\n",
      "tanh: \n",
      "epoch 1/35   error=0.111987\n",
      "epoch 35/35   error=0.020459\n",
      "Accuracy: 0.73812\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "loss = Loss(loss_function, loss_function_derivative)\n",
    "\n",
    "#netReLU\n",
    "netReLU = Network([], 0.01)\n",
    "netReLU.add(FullyConnected(28*28, 100))\n",
    "\n",
    "netReLU.add(Tanh())\n",
    "netReLU.add(FullyConnected(100, 50))\n",
    "\n",
    "netReLU.add(Tanh())\n",
    "netReLU.add(FullyConnected(50, 10))\n",
    "\n",
    "netReLU.add(Tanh())\n",
    "\n",
    "#netTanh\n",
    "netTanh = Network([], 0.01)\n",
    "netTanh.add(FullyConnected(28*28, 100))\n",
    "\n",
    "netTanh.add(Tanh())\n",
    "netTanh.add(FullyConnected(100, 50))\n",
    "\n",
    "netTanh.add(Tanh())\n",
    "netTanh.add(FullyConnected(50, 10))\n",
    "\n",
    "netTanh.add(Tanh())\n",
    "\n",
    "print(\"reLU: \")\n",
    "netReLU.fit(x_train[0:1000], y_train[0:1000], 35, 0.01, loss)\n",
    "\n",
    "# test on samples\n",
    "out = net.predict(x_test)\n",
    "\n",
    "predicted_values = []\n",
    "for sample in out:\n",
    "    for num in sample:\n",
    "        for el in num:\n",
    "            if el >= 0.5:\n",
    "                predicted_values.append(1)\n",
    "            else:\n",
    "                predicted_values.append(0)\n",
    "\n",
    "y_values = []\n",
    "for sample in y_test:\n",
    "    for el in sample:\n",
    "        if el == 1:\n",
    "            y_values.append(1)\n",
    "        else:\n",
    "            y_values.append(0)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_values, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")\n",
    "\n",
    "print(\"tanh: \")\n",
    "netTanh.fit(x_train[0:1000], y_train[0:1000], 35, 0.01, loss)\n",
    "\n",
    "# test on samples\n",
    "out = net.predict(x_test)\n",
    "\n",
    "predicted_values = []\n",
    "for sample in out:\n",
    "    for num in sample:\n",
    "        for el in num:\n",
    "            if el >= 0.5:\n",
    "                predicted_values.append(1)\n",
    "            else:\n",
    "                predicted_values.append(0)\n",
    "\n",
    "y_values = []\n",
    "for sample in y_test:\n",
    "    for el in sample:\n",
    "        if el == 1:\n",
    "            y_values.append(1)\n",
    "        else:\n",
    "            y_values.append(0)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_values, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 small layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.223152\n",
      "epoch 35/35   error=0.035549\n",
      "Accuracy: 0.95282\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "loss = Loss(loss_function, loss_function_derivative)\n",
    "\n",
    "net = Network([], 0.01)\n",
    "net.add(FullyConnected(28*28, 20)) #input layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(20, 20)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(20, 10)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(10, 10)) #output layer\n",
    "\n",
    "net.add(Tanh())\n",
    "\n",
    "net.fit(x_train[0:1000], y_train[0:1000], 35, 0.01, loss)\n",
    "\n",
    "# test on samples\n",
    "out = net.predict(x_test)\n",
    "\n",
    "predicted_values = []\n",
    "for sample in out:\n",
    "    for num in sample:\n",
    "        for el in num:\n",
    "            if el >= 0.5:\n",
    "                predicted_values.append(1)\n",
    "            else:\n",
    "                predicted_values.append(0)\n",
    "\n",
    "y_values = []\n",
    "for sample in y_test:\n",
    "    for el in sample:\n",
    "        if el == 1:\n",
    "            y_values.append(1)\n",
    "        else:\n",
    "            y_values.append(0)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_values, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 extensive layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.100090\n",
      "epoch 35/35   error=0.020416\n",
      "Accuracy: 0.97016\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "loss = Loss(loss_function, loss_function_derivative)\n",
    "\n",
    "net = Network([], 0.01)\n",
    "net.add(FullyConnected(28*28, 500)) #input layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 100)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(100, 50)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(50, 10)) #output layer\n",
    "\n",
    "net.add(Tanh())\n",
    "\n",
    "net.fit(x_train[0:1000], y_train[0:1000], 35, 0.01, loss)\n",
    "\n",
    "# test on samples\n",
    "out = net.predict(x_test)\n",
    "\n",
    "predicted_values = []\n",
    "for sample in out:\n",
    "    for num in sample:\n",
    "        for el in num:\n",
    "            if el >= 0.5:\n",
    "                predicted_values.append(1)\n",
    "            else:\n",
    "                predicted_values.append(0)\n",
    "\n",
    "y_values = []\n",
    "for sample in y_test:\n",
    "    for el in sample:\n",
    "        if el == 1:\n",
    "            y_values.append(1)\n",
    "        else:\n",
    "            y_values.append(0)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_values, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 extensive layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.101556\n",
      "epoch 35/35   error=0.041005\n",
      "Accuracy: 0.92151\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "loss = Loss(loss_function, loss_function_derivative)\n",
    "\n",
    "net = Network([], 0.01)\n",
    "net.add(FullyConnected(28*28, 500)) #input layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 500)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 500)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 500)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 500)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 500)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(500, 250)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(250, 100)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(100, 50)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(50, 50)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(50, 50)) #hidden layer\n",
    "\n",
    "net.add(Tanh())\n",
    "net.add(FullyConnected(50, 10)) #output layer\n",
    "\n",
    "net.add(Tanh())\n",
    "\n",
    "net.fit(x_train[0:1000], y_train[0:1000], 35, 0.01, loss)\n",
    "\n",
    "# test on samples\n",
    "out = net.predict(x_test)\n",
    "\n",
    "predicted_values = []\n",
    "for sample in out:\n",
    "    for num in sample:\n",
    "        for el in num:\n",
    "            if el >= 0.5:\n",
    "                predicted_values.append(1)\n",
    "            else:\n",
    "                predicted_values.append(0)\n",
    "\n",
    "y_values = []\n",
    "for sample in y_test:\n",
    "    for el in sample:\n",
    "        if el == 1:\n",
    "            y_values.append(1)\n",
    "        else:\n",
    "            y_values.append(0)\n",
    "\n",
    "score_pred = sklearn.metrics.accuracy_score(y_values, predicted_values)\n",
    "print(f\"Accuracy: {score_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wnioski"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Zwiększanie liczby neuronów nie zapewnia poprawy wyników, dobór najlepszej ilości neuronów w warstwie ukrytej dokonywany jest w sposób empiryczny.\n",
    "\n",
    "2. Dobór wartości współczynnika uczenia w sposób znaczący wpływa na działanie sieci, \n",
    "zbyt duży współczynnik może spowodować że wyniki będą oscylować wokół oczekiwanej wartości, natomiast zbyt mała wartość spowalnia proces uczenia, wg przeprowadzonych\n",
    "eksperymentów proponowaną przez nas wartością współczynnika uczenia jest 0.01.\n",
    "\n",
    "3. W przypadku kiedy ilośc itearcji epochs jest zbyt mała dokładność nie jest zadowalająca, sieć działa poprawnie ( błąd jest znikomy) \n",
    "dla epoches w przedziale od 35 do 50 iteracji.\n",
    "\n",
    "4. Głębokość sieci oraz ilość wejść do neuronów znacznie wpływają na czas działania algorytmu, program działa w akceptowalnym czasie dla wejść nie \n",
    "przekraczających 500 wejść w pierwszej warstwie sieci."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f75e0feec92dee3b009653d57e81dbac4950bad359ee4977b1aa3016ac007f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
